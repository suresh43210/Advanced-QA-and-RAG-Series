{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTNQwD9tJaKQ"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4x3qUn1AJaKT",
        "outputId": "70f249c0-2e76-4c93-db24-2a5c280e2893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.3/981.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \\\n",
        "    ragstack-ai-langchain[knowledge-store]==1.3.0 \\\n",
        "    beautifulsoup4 markdownify python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQZnkHBsJaKT"
      },
      "source": [
        "# Load the Astra Documentation into Graph Store\n",
        "\n",
        "First, we'll crawl the DataStax documentation. LangChain includes a `SiteMapLoader` but it loads all of the pages into memory simultaneously, which makes it impossible to index larger sites from small environments (such as CoLab). So, we'll scrape the sitemap ourselves and iterate over the URLs, allowing us to process documents in batches and flush them to Astra DB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DhUDTiPJaKU"
      },
      "source": [
        "## Scrape the URLs from the Site Maps\n",
        "First, we use Beautiful Soup to parse the XML content of each sitemap and get the list of URLs.\n",
        "We also add a few extra URLs for external sites that are also useful to include in the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UlyaUIKmJaKU",
        "outputId": "572d8f82-5582-423e-9edb-39a737fb384d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2452"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Use sitemaps to crawl the content\n",
        "SITEMAPS = [\n",
        "    \"https://docs.datastax.com/en/sitemap-astra-db-vector.xml\",\n",
        "    \"https://docs.datastax.com/en/sitemap-cql.xml\",\n",
        "    \"https://docs.datastax.com/en/sitemap-dev-app-drivers.xml\",\n",
        "    \"https://docs.datastax.com/en/sitemap-glossary.xml\",\n",
        "    \"https://docs.datastax.com/en/sitemap-astra-db-serverless.xml\",\n",
        "]\n",
        "\n",
        "# Additional URLs to crawl for content.\n",
        "EXTRA_URLS = [\"https://github.com/jbellis/jvector\"]\n",
        "\n",
        "SITE_PREFIX = \"astra\"\n",
        "\n",
        "\n",
        "def load_pages(sitemap_url):\n",
        "    r = requests.get(\n",
        "        sitemap_url,\n",
        "        headers={\n",
        "            # Astra docs only return a sitemap with a user agent set.\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:58.0) Gecko/20100101 \"\n",
        "            \"Firefox/58.0\",\n",
        "        },\n",
        "        timeout=30,\n",
        "    )\n",
        "    xml = r.text\n",
        "\n",
        "    soup = BeautifulSoup(xml, features=\"xml\")\n",
        "    url_tags = soup.find_all(\"url\")\n",
        "    for url in url_tags:\n",
        "        yield (url.find(\"loc\").text)\n",
        "\n",
        "\n",
        "# For maintenance purposes, we could check only the new articles since a given time.\n",
        "URLS = [url for sitemap_url in SITEMAPS for url in load_pages(sitemap_url)] + EXTRA_URLS\n",
        "len(URLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcSm_-2bJaKU"
      },
      "source": [
        "## Load the content from each URL\n",
        "Next, we create the code to load each page. This performs the following steps:\n",
        "\n",
        "1. Parses the HTML with BeautifulSoup\n",
        "2. Locates the \"content\" of the HTML using an appropriate selector based on the URL\n",
        "3. Find the link (`<a href=\"...\">`) tags in the content and collect the absolute URLs (for creating edges).\n",
        "\n",
        "Adding the URLs of these references to the metadata allows the graph store to create edges between the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c4uL-zP1JaKV",
        "outputId": "93f2b02c-d992-4158-dd0d-c7f93b39d9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from typing import AsyncIterator, Iterable\n",
        "\n",
        "from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "from langchain_core.documents import Document\n",
        "from markdownify import MarkdownConverter\n",
        "from ragstack_knowledge_store.graph_store import CONTENT_ID\n",
        "from ragstack_langchain.graph_store.extractors import HtmlInput, HtmlLinkExtractor\n",
        "from ragstack_langchain.graph_store.links import add_links\n",
        "\n",
        "markdown_converter = MarkdownConverter(heading_style=\"ATX\")\n",
        "html_link_extractor = HtmlLinkExtractor()\n",
        "\n",
        "\n",
        "def select_content(soup: BeautifulSoup, url: str) -> BeautifulSoup:\n",
        "    if url.startswith(\"https://docs.datastax.com/en/\"):\n",
        "        return soup.select_one(\"article.doc\")\n",
        "    if url.startswith(\"https://github.com\"):\n",
        "        return soup.select_one(\"article.entry-content\")\n",
        "    return soup\n",
        "\n",
        "\n",
        "async def load_pages(urls: Iterable[str]) -> AsyncIterator[Document]:\n",
        "    loader = AsyncHtmlLoader(\n",
        "        urls,\n",
        "        requests_per_second=4,\n",
        "        # Astra docs require a user agent\n",
        "        header_template={\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:58.0) Gecko/20100101 \"\n",
        "            \"Firefox/58.0\"\n",
        "        },\n",
        "    )\n",
        "    async for html in loader.alazy_load():\n",
        "        url = html.metadata[\"source\"]\n",
        "\n",
        "        # Use the URL as the content ID.\n",
        "        html.metadata[CONTENT_ID] = url\n",
        "\n",
        "        # Apply the selectors while loading. This reduces the size of\n",
        "        # the document as early as possible for reduced memory usage.\n",
        "        soup = BeautifulSoup(html.page_content, \"html.parser\")\n",
        "        content = select_content(soup, url)\n",
        "\n",
        "        # Extract HTML links from the content.\n",
        "        add_links(html, html_link_extractor.extract_one(HtmlInput(content, url)))\n",
        "\n",
        "        # Convert the content to markdown\n",
        "        html.page_content = markdown_converter.convert_soup(content)\n",
        "\n",
        "        yield html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvyRKsOvJaKV"
      },
      "source": [
        "## Initialize Environment\n",
        "Before we initialize the Graph Store and write the documents we need to set some environment variables.\n",
        "In colab, this will prompt you for input. When running locally, this will load from `.env`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h1XikFDFJaKV",
        "outputId": "69d03b4a-885e-486f-aa60-7860db376e6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In colab. Using getpass/input for environment variables.\n",
            "Enter OpenAI API Key: ··········\n",
            "Enter Astra DB Database ID: 0ab11e24-a52e-4327-b333-4b280ddfd46e\n",
            "Enter Astra DB Application Token: ··········\n",
            "Enter Astra DB Keyspace (Empty for default): \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    # (Option 1) - Set the environment variables from getpass.\n",
        "    print(\"In colab. Using getpass/input for environment variables.\")\n",
        "    import getpass\n",
        "    import os\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
        "    os.environ[\"ASTRA_DB_DATABASE_ID\"] = input(\"Enter Astra DB Database ID: \")\n",
        "    os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass.getpass(\n",
        "        \"Enter Astra DB Application Token: \"\n",
        "    )\n",
        "\n",
        "    keyspace = input(\"Enter Astra DB Keyspace (Empty for default): \")\n",
        "    if keyspace:\n",
        "        os.environ[\"ASTRA_DB_KEYSPACE\"] = keyspace\n",
        "    else:\n",
        "        os.environ.pop(\"ASTRA_DB_KEYSPACE\", None)\n",
        "else:\n",
        "    print(\"Not in colab. Loading '.env' (see 'env.template' for example)\")\n",
        "    import dotenv\n",
        "\n",
        "    dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJIvCHrRJaKW"
      },
      "source": [
        "## Initialize Cassio and Graph Store\n",
        "With the environment variables set we initialize the Cassio library for talking to Cassandra / Astra DB.\n",
        "We also create the `GraphStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KiFyI088JaKW"
      },
      "outputs": [],
      "source": [
        "SITE_PREFIX = \"astra_docs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iLOwXOu0JaKW",
        "outputId": "3080c9ab-5374-4861-a8fe-3ee2243bc32c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drop Tables? [(Y)es/(N)o]n\n"
          ]
        }
      ],
      "source": [
        "answer = input(\"Drop Tables? [(Y)es/(N)o]\")\n",
        "if answer.lower() in [\"y\", \"yes\"]:\n",
        "    import cassio\n",
        "\n",
        "    cassio.init(auto=True)\n",
        "    from cassio.config import check_resolve_keyspace, check_resolve_session\n",
        "\n",
        "    session = check_resolve_session()\n",
        "    keyspace = check_resolve_keyspace()\n",
        "    session.execute(f\"DROP TABLE IF EXISTS {keyspace}.{SITE_PREFIX}_nodes\")\n",
        "    session.execute(f\"DROP TABLE IF EXISTS {keyspace}.{SITE_PREFIX}_targets\")\n",
        "else:\n",
        "    # Handle no / \"wrong\" input\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Au2obgQ0JaKW"
      },
      "outputs": [],
      "source": [
        "import cassio\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragstack_langchain.graph_store import CassandraGraphStore\n",
        "\n",
        "cassio.init(auto=True)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "graph_store = CassandraGraphStore(\n",
        "    embeddings,\n",
        "    node_table=f\"{SITE_PREFIX}_nodes\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y294ydRJaKW"
      },
      "source": [
        "## Load the Documents\n",
        "Finally, we fetch pages and write them to the graph store in batches of 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JkaMP61XJaKX",
        "outputId": "ccde6d43-a2cc-4186-d079-44c16ed9fc53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages:   6%|6         | 149/2452 [00:54<14:20,  2.68it/s]WARNING:cassandra.protocol:Server warning: Detected collection link_to_tags with 21 items, greater than the maximum recommended (20)\n",
            "Fetching pages:  77%|#######7  | 1897/2452 [12:45<03:07,  2.96it/s]WARNING:cassandra.protocol:Server warning: Detected collection link_to_tags with 23 items, greater than the maximum recommended (20)\n",
            "Fetching pages: 100%|##########| 2452/2452 [16:08<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 (of 2452) URLs were not found\n"
          ]
        }
      ],
      "source": [
        "not_found = 0\n",
        "found = 0\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "docs = []\n",
        "async for doc in load_pages(URLS):\n",
        "    if doc.page_content.startswith(\"\\n# Page Not Found\"):\n",
        "        not_found += 1\n",
        "        continue\n",
        "\n",
        "    docs.append(doc)\n",
        "    found += 1\n",
        "\n",
        "    if len(docs) >= BATCH_SIZE:\n",
        "        graph_store.add_documents(docs)\n",
        "        docs.clear()\n",
        "\n",
        "if docs:\n",
        "    graph_store.add_documents(docs)\n",
        "print(f\"{not_found} (of {not_found + found}) URLs were not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-MHafhJaKX"
      },
      "source": [
        "# Create and execute the RAG Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WOhR3bSKJaKX"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "template = \"\"\"You are a helpful technical support bot. You should provide complete answers explaining the options the user has available to address their problem. Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"  # noqa: E501\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"From {doc.metadata['content_id']}: {doc.page_content}\" for doc in docs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9Le8eDJaKX"
      },
      "source": [
        "We'll use the following question. This is an interesting question because the ideal answer should be concise and in-depth, based on how the vector indexing is actually implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oWnZ1eJnJaKX"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"What vector indexing algorithms does Astra use?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0pmjAN4uJaKX"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "# Helper method to render markdown in responses to a chain.\n",
        "def run_and_render(chain, question) -> None:\n",
        "    result = chain.invoke(question)\n",
        "    display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCiQGtFDJaKX"
      },
      "source": [
        "## Vector-Only Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aPtAf7CoJaKY"
      },
      "outputs": [],
      "source": [
        "# Depth 0 doesn't traverses edges and is equivalent to vector similarity only.\n",
        "vector_retriever = graph_store.as_retriever(search_kwargs={\"depth\": 0})\n",
        "\n",
        "vector_rag_chain = (\n",
        "    {\"context\": vector_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PEWTirtuJaKY",
        "outputId": "b098ed7d-de42-436f-e22d-b65cca14f235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Astra DB uses two main vector indexing algorithms:\n\n1. **JVector**: This is a vector search engine used by Serverless (Vector) databases to construct graph indexes. JVector allows for efficient search by adding new documents to the graph immediately. It can also compress vectors with quantization to save space and improve performance.\n\n2. **Storage-Attached Indexing (SAI)**: SAI is an indexing technique that efficiently finds rows that satisfy query predicates. Astra DB provides numeric, text, and vector-based indexes to support different kinds of searches. SAI evaluates search criteria, sorts results by vector similarity, and returns the top results based on the predicates provided.\n\nThese indexing algorithms enable efficient and scalable vector search capabilities within Astra DB."
          },
          "metadata": {}
        }
      ],
      "source": [
        "run_and_render(vector_rag_chain, QUESTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uckMOt0RJaKY"
      },
      "source": [
        "## Graph Traversal Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "S-vryxWJJaKY"
      },
      "outputs": [],
      "source": [
        "# Depth 1 does vector similarity and then traverses 1 level of edges.\n",
        "graph_retriever = graph_store.as_retriever(search_kwargs={\"depth\": 1})\n",
        "\n",
        "graph_rag_chain = (\n",
        "    {\"context\": graph_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "source": [
        "# Added k to limit the number of retrieved documents\n",
        "graph_retriever = graph_store.as_retriever(search_kwargs={\"depth\": 1, \"k\": 1}) # Reduced from default of 4\n",
        "\n",
        "graph_rag_chain = (\n",
        "    {\"context\": graph_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "umbRuahjbpyX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ve6g9B_JaKY"
      },
      "source": [
        "## MMR Graph Traversal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKSRefALJaKY"
      },
      "outputs": [],
      "source": [
        "run_and_render(graph_rag_chain, QUESTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DrXff2NmJaKY"
      },
      "outputs": [],
      "source": [
        "mmr_graph_retriever = graph_store.as_retriever(\n",
        "    search_type=\"mmr_traversal\",\n",
        "    search_kwargs={\n",
        "        \"k\": 4,\n",
        "        \"fetch_k\": 10,\n",
        "        \"depth\": 2,\n",
        "        # \"score_threshold\": 0.2,\n",
        "    },\n",
        ")\n",
        "\n",
        "mmr_graph_rag_chain = (\n",
        "    {\"context\": mmr_graph_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWQo9odYJaKY"
      },
      "outputs": [],
      "source": [
        "run_and_render(mmr_graph_rag_chain, QUESTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2vVBatTJaKZ"
      },
      "source": [
        "## Check Retrieval Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hweuKNmJaKZ"
      },
      "outputs": [],
      "source": [
        "# Set the question and see what documents each technique retrieves.\n",
        "for i, doc in enumerate(vector_retriever.invoke(QUESTION)):\n",
        "    print(f\"Vector [{i}]:    {doc.metadata['content_id']}\")\n",
        "\n",
        "for i, doc in enumerate(graph_retriever.invoke(QUESTION)):\n",
        "    print(f\"Graph [{i}]:     {doc.metadata['content_id']}\")\n",
        "\n",
        "for i, doc in enumerate(mmr_graph_retriever.invoke(QUESTION)):\n",
        "    print(f\"MMR Graph [{i}]: {doc.metadata['content_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUkHHvrNJaKZ"
      },
      "source": [
        "# Conclusion\n",
        "With vector only we retrieved chunks from the Astra documentation explaining that it used JVector.\n",
        "Since it didn't follow the link to [JVector on GitHub](https://github.com/jbellis/jvector) it didn't actually answer the question.\n",
        "\n",
        "The graph retrieval started with the same set of chunks, but it followed the edge to the documents we loaded from GitHub.\n",
        "This allowed the LLM to read in more depth how JVector is implemented, which allowed it to answer the question more clearly and with more detail."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agent-framework-aiP65pJh-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}